#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-4B-Instruct-2507 æœƒè­°è¨˜éŒ„æ•´ç†åŠ©æ‰‹ (è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬)
å°ˆé–€ç”¨æ–¼è™•ç†CSVæ–‡ä»¶ä¸­çš„æœƒè­°è¨˜éŒ„ä¸¦é€²è¡Œé€è¡Œé‡é»æ•´ç†ï¼Œæœ€å¾Œç¸½çµæ•´å€‹æœƒè­°ä¸»é¡Œ
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings
import pandas as pd
from datetime import datetime
import os
import gc

warnings.filterwarnings("ignore")

class Qwen3MeetingRecordExtractor:
    """
    ä½¿ç”¨Qwen3-4B-Instruct-2507æ¨¡å‹å°ˆé–€é€²è¡Œæœƒè­°è¨˜éŒ„æ•´ç†çš„åŠ©æ‰‹é¡ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ï¼‰
    """

    def __init__(self, model_name="Qwen/Qwen3-4B-Instruct-2507", device_map="auto", token=None):
        """
        åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        """
        print("æ­£åœ¨è¼‰å…¥Qwen3-4B-Instruct-2507æ¨¡å‹ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ï¼‰...")
        print("æ³¨æ„ï¼šé¦–æ¬¡è¼‰å…¥å¯èƒ½éœ€è¦æ•¸åˆ†é˜æ™‚é–“ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ")

        # è¨­ç½®æˆæ¬ŠToken
        token = "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"

        # è¼‰å…¥tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            use_auth_token=token
        )

        # è¼‰å…¥æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map=device_map,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_auth_token=token
        )

        # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
        self.model.eval()

        print(f"æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        print(f"æ¨¡å‹è¨­å‚™: {self.model.device}")
        print(f"æ¨¡å‹ç²¾åº¦: {self.model.dtype}")

    def create_meeting_prompt(self, text):
        """
        å‰µå»ºæœƒè­°è¨˜éŒ„æ•´ç†å°ˆç”¨prompt
        """
        prompt_template = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„æ•´ç†å°ˆå®¶ï¼Œæ“…é•·å¾æœƒè­°è¨˜éŒ„ä¸­æå–é—œéµä¿¡æ¯ã€‚

### ä»»å‹™èªªæ˜ ###
è«‹å°ä»¥ä¸‹æœƒè­°è¨˜éŒ„é€²è¡Œé‡é»æ•´ç†ï¼Œè¦æ±‚ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
2. æå–æœ€å¤š2å€‹æ ¸å¿ƒè¦é»
3. å€åˆ†æ±ºç­–äº‹é …ã€è¡Œå‹•é …ç›®å’Œè¨è«–é‡é»
4. æ¯å€‹è¦é»æ‡‰è©²ç°¡æ½”æ˜äº†ï¼Œçªå‡ºé—œéµä¿¡æ¯
5. æŒ‰é‡è¦æ€§æ’åº

### è¼¸å‡ºæ ¼å¼ ###
è«‹æŒ‰ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š

## æœƒè­°è¨˜éŒ„æ•´ç†

### æ ¸å¿ƒè¦é»
1. [é—œéµè¦é»] - ç°¡è¦èªªæ˜
2. [é—œéµè¦é»] - ç°¡è¦èªªæ˜

### æ±ºç­–äº‹é …
- [æ±ºç­–å…§å®¹]ï¼ˆå¦‚æœ‰ï¼‰

### è¡Œå‹•é …ç›®
- [å¾…è¾¦äº‹é …]ï¼ˆå¦‚æœ‰ï¼‰

### ç¸½çµ
[ä¸€å¥è©±ç¸½çµæœ¬æ®µæœƒè­°å…§å®¹çš„æ ¸å¿ƒ]

### å¾…æ•´ç†çš„æœƒè­°è¨˜éŒ„ ###
{text}

è«‹é–‹å§‹æ•´ç†ï¼š"""

        return prompt_template

    def create_compact_summary_prompt(self, key_themes, decisions, actions, total_records):
        """
        å‰µå»ºç²¾ç°¡ç‰ˆæ•´é«”æœƒè­°ç¸½çµpromptï¼ˆè¨˜æ†¶é«”å„ªåŒ–ï¼‰
        Args:
            key_themes: é—œéµä¸»é¡Œåˆ—è¡¨
            decisions: æ±ºç­–äº‹é …åˆ—è¡¨  
            actions: è¡Œå‹•é …ç›®åˆ—è¡¨
            total_records: ç¸½è¨˜éŒ„æ•¸
        """
        themes_text = "\n".join([f"- {theme}" for theme in key_themes[:10]])  # é™åˆ¶ä¸»é¡Œæ•¸é‡
        decisions_text = "\n".join([f"- {decision}" for decision in decisions[:8]])  # é™åˆ¶æ±ºç­–æ•¸é‡
        actions_text = "\n".join([f"- {action}" for action in actions[:8]])  # é™åˆ¶è¡Œå‹•é …ç›®æ•¸é‡

        prompt_template = f"""ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æœƒè­°åˆ†æå°ˆå®¶ï¼Œè«‹åŸºæ–¼ä»¥ä¸‹æå–çš„é—œéµä¿¡æ¯ï¼Œç¸½çµæ•´å€‹æœƒè­°çš„æ ¸å¿ƒä¸»é¡Œã€‚

### æœƒè­°åŸºæœ¬ä¿¡æ¯ ###
- ç¸½ç™¼è¨€æ®µæ•¸ï¼š{total_records} æ®µ
- åˆ†ææ—¥æœŸï¼š{datetime.now().strftime('%Y-%m-%d')}

### æå–çš„é—œéµä¸»é¡Œ ###
{themes_text}

### é‡è¦æ±ºç­–äº‹é … ###
{decisions_text}

### è¡Œå‹•é …ç›® ###
{actions_text}

### ä»»å‹™è¦æ±‚ ###
è«‹åŸºæ–¼ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç°¡æ½”çš„æ•´é«”æœƒè­°ç¸½çµï¼š

## ğŸ¯ æœƒè­°æ•´é«”ä¸»é¡Œç¸½çµ

### æœƒè­°æ ¸å¿ƒä¸»é¡Œ
[ç”¨1-2å¥è©±æ¦‚æ‹¬æ•´å ´æœƒè­°çš„ä¸»è¦ç›®çš„å’Œæ ¸å¿ƒè­°é¡Œ]

### ä¸»è¦è¨è«–ç„¦é»
1. [ç„¦é»ä¸€] - ç°¡è¦èªªæ˜
2. [ç„¦é»äºŒ] - ç°¡è¦èªªæ˜
3. [ç„¦é»ä¸‰] - ç°¡è¦èªªæ˜

### é‡è¦æˆæœ
- [æˆæœä¸€]
- [æˆæœäºŒ]
- [æˆæœä¸‰]

### å¾ŒçºŒè¡Œå‹•
- [è¡Œå‹•ä¸€]
- [è¡Œå‹•äºŒ]
- [è¡Œå‹•ä¸‰]

### æœƒè­°æ„ç¾©
[2å¥è©±ç¸½çµé€™æ¬¡æœƒè­°çš„é‡è¦æ€§å’Œå½±éŸ¿]

è«‹é–‹å§‹ç¸½çµï¼š"""

        return prompt_template

    def clean_memory(self):
        """
        æ¸…ç†GPUè¨˜æ†¶é«”
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def generate_meeting_summary(self, text, max_tokens=512, temperature=0.7, top_p=0.8, top_k=20):
        """
        ç”Ÿæˆæœƒè­°è¨˜éŒ„é‡é»æ•´ç†
        """
        # å‰µå»ºæœƒè­°è¨˜éŒ„å°ˆç”¨prompt
        prompt = self.create_meeting_prompt(text)

        # æ§‹å»ºæ¶ˆæ¯æ ¼å¼
        messages = [
            {"role": "user", "content": prompt}
        ]

        # æ‡‰ç”¨èŠå¤©æ¨¡æ¿
        text_input = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        # ç·¨ç¢¼è¼¸å…¥
        model_inputs = self.tokenizer([text_input], return_tensors="pt")

        # ç§»å‹•åˆ°æ¨¡å‹è¨­å‚™
        if hasattr(self.model, 'device'):
            model_inputs = {k: v.to(self.model.device) for k, v in model_inputs.items()}

        # ç”Ÿæˆå›æ‡‰
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # è§£ç¢¼è¼¸å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        output_ids = generated_ids[0][len(model_inputs['input_ids'][0]):].tolist()
        summary = self.tokenizer.decode(output_ids, skip_special_tokens=True)

        # æ¸…ç†è¨˜æ†¶é«”
        del model_inputs, generated_ids
        self.clean_memory()

        return summary.strip()

    def extract_key_elements(self, all_summaries):
        """
        å¾æ‰€æœ‰æ‘˜è¦ä¸­æå–é—œéµå…ƒç´ ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ï¼‰
        """
        key_themes = []
        decisions = []
        actions = []

        for summary in all_summaries:
            # ç°¡å–®æå–é—œéµä¿¡æ¯
            lines = summary.split('\n')
            current_section = ""

            for line in lines:
                line = line.strip()
                if "### æ ¸å¿ƒè¦é»" in line:
                    current_section = "themes"
                elif "### æ±ºç­–äº‹é …" in line:
                    current_section = "decisions"  
                elif "### è¡Œå‹•é …ç›®" in line:
                    current_section = "actions"
                elif "### ç¸½çµ" in line:
                    current_section = "summary"
                elif line and line.startswith(('-', '1.', '2.', '3.')):
                    if current_section == "themes" and len(key_themes) < 15:
                        key_themes.append(line.lstrip('- 123.').strip())
                    elif current_section == "decisions" and len(decisions) < 10:
                        decisions.append(line.lstrip('- ').strip())
                    elif current_section == "actions" and len(actions) < 10:
                        actions.append(line.lstrip('- ').strip())

        return key_themes, decisions, actions

    def generate_compact_summary(self, key_themes, decisions, actions, total_records):
        """
        ç”Ÿæˆç²¾ç°¡ç‰ˆæ•´é«”ç¸½çµï¼ˆè¨˜æ†¶é«”å„ªåŒ–ï¼‰
        """
        print("\næ­£åœ¨ç”Ÿæˆæ•´é«”æœƒè­°ä¸»é¡Œç¸½çµï¼ˆè¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼ï¼‰...")

        # å‰µå»ºç²¾ç°¡ç‰ˆprompt
        prompt = self.create_compact_summary_prompt(key_themes, decisions, actions, total_records)

        # æ§‹å»ºæ¶ˆæ¯æ ¼å¼
        messages = [
            {"role": "user", "content": prompt}
        ]

        # æ‡‰ç”¨èŠå¤©æ¨¡æ¿
        text_input = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        # ç·¨ç¢¼è¼¸å…¥
        model_inputs = self.tokenizer([text_input], return_tensors="pt")

        # ç§»å‹•åˆ°æ¨¡å‹è¨­å‚™
        if hasattr(self.model, 'device'):
            model_inputs = {k: v.to(self.model.device) for k, v in model_inputs.items()}

        print("æ­£åœ¨ç”Ÿæˆæ•´é«”ç¸½çµ...")

        # ç”Ÿæˆå›æ‡‰
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=800,  # æ¸›å°‘è¼¸å‡ºé•·åº¦
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # è§£ç¢¼è¼¸å‡º
        output_ids = generated_ids[0][len(model_inputs['input_ids'][0]):].tolist()
        overall_summary = self.tokenizer.decode(output_ids, skip_special_tokens=True)

        # æ¸…ç†è¨˜æ†¶é«”
        del model_inputs, generated_ids
        self.clean_memory()

        return overall_summary.strip()

    def process_csv_file(self, csv_file_path, output_file_path=None):
        """
        è™•ç†CSVæ–‡ä»¶ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ï¼‰
        """
        try:
            # è®€å–CSVæ–‡ä»¶
            print(f"æ­£åœ¨è®€å–CSVæ–‡ä»¶: {csv_file_path}")
            df = pd.read_csv(csv_file_path, encoding='utf-8')

            # æª¢æŸ¥æ˜¯å¦æœ‰'åŸæ–‡'æ¬„ä½
            if 'åŸæ–‡' not in df.columns:
                print("éŒ¯èª¤ï¼šCSVæ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°'åŸæ–‡'æ¬„ä½")
                return None

            print(f"æ‰¾åˆ° {len(df)} è¡Œæœƒè­°è¨˜éŒ„")
            print("ğŸ§  ä½¿ç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼è™•ç†...")

            # æº–å‚™çµæœåˆ—è¡¨
            results = []
            all_summaries = []
            total_original_chars = 0

            # é€è¡Œè™•ç†
            for index, row in df.iterrows():
                original_text = str(row['åŸæ–‡']).strip()

                # è·³éç©ºè¡Œæˆ–ç„¡æ•ˆå…§å®¹
                if not original_text or original_text == 'nan' or len(original_text) < 10:
                    print(f"ç¬¬ {index + 1} è¡Œå…§å®¹éçŸ­ï¼Œè·³éè™•ç†")
                    continue

                print(f"è™•ç†ç¬¬ {index + 1}/{len(df)} è¡Œ...")

                try:
                    # ç”Ÿæˆé‡é»æ•´ç†
                    summary = self.generate_meeting_summary(original_text)

                    # ç´¯åŠ å­—æ•¸çµ±è¨ˆ
                    total_original_chars += len(original_text)

                    # ä¿å­˜åˆ°ç¸½çµåˆ—è¡¨
                    all_summaries.append(summary)

                    # ä¿å­˜çµæœ
                    result = {
                        'è¡Œè™Ÿ': index + 1,
                        'åŸæ–‡': original_text,
                        'åŸæ–‡é•·åº¦': len(original_text),
                        'é‡é»æ•´ç†': summary,
                        'è™•ç†æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    # å¦‚æœåŸCSVæœ‰å…¶ä»–æ¬„ä½ï¼Œä¹Ÿä¸€ä½µä¿ç•™
                    for col in df.columns:
                        if col != 'åŸæ–‡' and col in row:
                            result[f'åŸå§‹_{col}'] = row[col]

                    results.append(result)

                    print(f"âœ“ ç¬¬ {index + 1} è¡Œè™•ç†å®Œæˆ")

                    # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
                    if index % 20 == 0:
                        self.clean_memory()

                except Exception as e:
                    print(f"âœ— ç¬¬ {index + 1} è¡Œè™•ç†å¤±æ•—: {str(e)}")
                    continue

            if not results:
                print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è¨˜éŒ„")
                return None

            print("-" * 80)

            # æå–é—œéµå…ƒç´ ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ï¼‰
            print("\nğŸ”„ æ­£åœ¨æå–æœƒè­°é—œéµå…ƒç´ ...")
            key_themes, decisions, actions = self.extract_key_elements(all_summaries)

            print(f"âœ“ æå–åˆ° {len(key_themes)} å€‹é—œéµä¸»é¡Œ")
            print(f"âœ“ æå–åˆ° {len(decisions)} å€‹æ±ºç­–äº‹é …")  
            print(f"âœ“ æå–åˆ° {len(actions)} å€‹è¡Œå‹•é …ç›®")

            # æ¸…ç†åŸå§‹æ‘˜è¦ä»¥ç¯€çœè¨˜æ†¶é«”
            del all_summaries
            self.clean_memory()

            # ç”Ÿæˆç²¾ç°¡ç‰ˆæ•´é«”ç¸½çµ
            print("\n" + "="*80)
            print("é–‹å§‹ç”Ÿæˆæ•´é«”æœƒè­°ä¸»é¡Œç¸½çµ")
            print("="*80)

            overall_summary = self.generate_compact_summary(
                key_themes, decisions, actions, len(results)
            )

            print("\nâœ“ æ•´é«”æœƒè­°ä¸»é¡Œç¸½çµå®Œæˆï¼")

            # è½‰æ›ç‚ºDataFrame
            results_df = pd.DataFrame(results)

            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            if output_file_path is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file_path = f'meeting_summary_{timestamp}.csv'
                summary_file_path = f'meeting_overall_summary_{timestamp}.txt'
            else:
                base_name = output_file_path.replace('.csv', '')
                summary_file_path = f'{base_name}_overall_summary.txt'

            # ä¿å­˜é€è¡Œæ•´ç†çµæœ
            results_df.to_csv(output_file_path, index=False, encoding='utf-8-sig')
            print(f"\nâœ“ é€è¡Œæ•´ç†çµæœå·²ä¿å­˜è‡³: {output_file_path}")

            # ä¿å­˜æ•´é«”æœƒè­°ç¸½çµ
            with open(summary_file_path, 'w', encoding='utf-8') as f:
                f.write("# æœƒè­°æ•´é«”ä¸»é¡Œç¸½çµï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ï¼‰\n\n")
                f.write(f"**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**æœƒè­°è¨˜éŒ„ç¸½æ•¸**: {len(results)} æ®µ\n")
                f.write(f"**ç¸½å­—æ•¸**: {total_original_chars:,} å­—\n")
                f.write(f"**æå–é—œéµä¸»é¡Œæ•¸**: {len(key_themes)}\n")
                f.write(f"**æ±ºç­–äº‹é …æ•¸**: {len(decisions)}\n")
                f.write(f"**è¡Œå‹•é …ç›®æ•¸**: {len(actions)}\n\n")
                f.write("---\n\n")
                f.write(overall_summary)

            print(f"âœ“ æ•´é«”æœƒè­°ä¸»é¡Œç¸½çµå·²ä¿å­˜è‡³: {summary_file_path}")
            print(f"âœ“ æˆåŠŸè™•ç† {len(results)} æ¢è¨˜éŒ„")

            # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
            self.show_processing_stats(results_df, overall_summary)

            return results_df, overall_summary

        except Exception as e:
            print(f"è™•ç†CSVæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None

    def show_processing_stats(self, results_df, overall_summary=None):
        """
        é¡¯ç¤ºè™•ç†çµ±è¨ˆä¿¡æ¯
        """
        print("\n" + "="*60)
        print("è™•ç†çµ±è¨ˆ")
        print("="*60)

        if len(results_df) > 0:
            avg_original_length = results_df['åŸæ–‡é•·åº¦'].mean()
            total_original_chars = results_df['åŸæ–‡é•·åº¦'].sum()

            print(f"ç¸½è™•ç†è¡Œæ•¸: {len(results_df)}")
            print(f"å¹³å‡åŸæ–‡é•·åº¦: {avg_original_length:.0f} å­—")
            print(f"ç¸½åŸæ–‡å­—æ•¸: {total_original_chars:,} å­—")
            print(f"æœ€é•·åŸæ–‡: {results_df['åŸæ–‡é•·åº¦'].max()} å­—")
            print(f"æœ€çŸ­åŸæ–‡: {results_df['åŸæ–‡é•·åº¦'].min()} å­—")

            # é¡¯ç¤ºæ•´é«”ç¸½çµé è¦½
            if overall_summary:
                print("\n" + "-"*40)
                print("æ•´é«”æœƒè­°ä¸»é¡Œç¸½çµé è¦½:")
                print("-"*40)
                # åªé¡¯ç¤ºç¸½çµçš„å‰200å­—
                preview = overall_summary[:200] + "..." if len(overall_summary) > 200 else overall_summary
                print(preview)

        print("="*60)

def process_meeting_records():
    """
    ä¸»è¦è™•ç†å‡½æ•¸ï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ï¼‰
    """
    # CSVæ–‡ä»¶è·¯å¾‘
    csv_file = "C:\\Users\\cbes1\\Desktop\\meeting assistence\\meeting_record\\project_test_1.csv"

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_file):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file}")
        print("è«‹ç¢ºä¿CSVæ–‡ä»¶åœ¨ç•¶å‰ç›®éŒ„ä¸‹")
        return

    try:
        print("="*60)
        print("Qwen3-4B-Instruct-2507 æœƒè­°è¨˜éŒ„æ•´ç†ç³»çµ±")
        print("è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬ - è§£æ±ºCUDAè¨˜æ†¶é«”ä¸è¶³å•é¡Œ")
        print("="*60)

        # åˆå§‹åŒ–æ¨¡å‹
        extractor = Qwen3MeetingRecordExtractor()

        print(f"\né–‹å§‹è™•ç†CSVæ–‡ä»¶: {csv_file}")

        # è™•ç†CSVæ–‡ä»¶
        result = extractor.process_csv_file(csv_file)

        if result is not None:
            print("\nğŸ‰ æ‰€æœ‰æœƒè­°è¨˜éŒ„è™•ç†å®Œæˆï¼")
            print("ğŸ“Š è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼æˆåŠŸé‹è¡Œ")
            print("âœ… å·²ç”Ÿæˆé€è¡Œæ•´ç†å’Œæ•´é«”ç¸½çµå…©å€‹æª”æ¡ˆ")
        else:
            print("\nâŒ è™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶æ ¼å¼å’Œå…§å®¹")

    except Exception as e:
        print(f"åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        print("\nå¦‚æœä»é‡åˆ°è¨˜æ†¶é«”å•é¡Œï¼Œè«‹å˜—è©¦ä»¥ä¸‹æ–¹æ³•ï¼š")
        print("1. é—œé–‰å…¶ä»–GPUæ‡‰ç”¨ç¨‹å¼")
        print("2. é‡å•ŸPythonç¨‹åºæ¸…ç†è¨˜æ†¶é«”")
        print("3. ä½¿ç”¨CPUç‰ˆæœ¬: device_map='cpu'")

if __name__ == "__main__":
    process_meeting_records()